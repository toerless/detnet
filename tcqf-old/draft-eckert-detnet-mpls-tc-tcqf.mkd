---
coding: utf-8

title: Deterministic Networking (DetNet) Data Plane - MPLS TC Tagging for Cyclic Queuing and Forwarding (MPLS-TC TCQF)
abbrev: detnet-mpls-tc-tcqf
docname: draft-eckert-detnet-mpls-tc-tcqf-03
wg: DETNET
category: std
ipr: trust200902
stream: IETF

stand_alone: yes
pi:
   toc: yes
   tocdepth: 5
   sortrefs: yes
   symrefs: yes
   comments: yes


author:
  -
       name: Toerless Eckert
       org: Futurewei Technologies USA
       street: 2220 Central Expressway
       city: Santa Clara
       code: CA 95050
       country: USA
       email: tte@cs.fau.de
  -
       name: Stewart Bryant
       org: University of Surrey ICS
       email: s.bryant@surrey.ac.uk
  -
       name: Andrew G. Malis
       org: Malis Consulting
       email: agmalis@gmail.com

normative:
  RFC3270:
  RFC8655:
  RFC8964:

informative:
  RFC3209:
  RFC8402:
  RFC9016:

  I-D.ietf-bier-te-arch:
  I-D.ietf-detnet-bounded-latency:
  I-D.eckert-detnet-bounded-latency-problems:
  I-D.qiang-DetNet-large-scale-DetNet:
  I-D.dang-queuing-with-multiple-cyclic-buffers:

  IEEE802.1Qbv:
    title: "IEEE Standard for Local and metropolitan area networks -- Bridges and Bridged Networks - Amendment 25: Enhancements for Scheduled Traffic"
    author:
      org: IEEE Time-Sensitive Networking (TSN) Task Group. 
    date: 2015

  IEEE802.1Qch:
    title: "IEEE Std 802.1Qch-2017: IEEE Standard for Local and Metropolitan Area Networks - Bridges and Bridged Networks - Amendment 29: Cyclic Queuing and Forwarding"
    author:
      org: IEEE Time-Sensitive Networking (TSN) Task Group. 
    date: 2017

  TSN-ATS:
    title: "P802.1Qcr - Bridges and Bridged Networks Amendment: Asynchronous Traffic Shaping"
    seriesinfo:
      IEEE: 
    date: 2020-07-09
    target: https://1.ieee802.org/tsn/802-1qcr/
    author:
      name: Johannes Specht

  LDN:
    title: "Towards Large-Scale Deterministic IP Networks"
    date: 2021
    seriesinfo:
      IEEE: "2021 IFIP Networking Conference (IFIP Networking)"
      doi: "10.23919/IFIPNetworking52078.2021.9472798"
    author:
      -
        name: Binyang Liu
      -
        name: Shoushou Ren
      -
        name: Chuang Wang
      -
        name: Vincent Angilella
      -
        name: Paolo Medagliani
      - 
        name: Sebastien Martin
      -
        name: Jeremie Leguay

--- abstract

This memo defines the use of the MPLS TC field of MPLS Label Stack Entries (LSE) to support
cycle tagging of packets for Multiple Buffer Cyclic Queuing and Forwarding (TCQF).
TCQF is a mechanism to support bounded latency forwarding in DetNet network.

Target benefits of TCQF include low end-to-end
jitter, ease of high-speed hardware implementation, optional ability to support large number
of flow in large networks via DiffServ style aggregation by applying TCQF to
the DetNet aggregate instead of each DetNet flow individually, and support of wide-area DetNet
networks with arbitrary link latencies and latency variations.

--- middle

# Introduction (informative)

Cyclic Queuing and Forwarding (CQF), {{IEEE802.1Qch}}, is an IEEE standardized queuing mechanism in support of deterministic bounded latency. See also {{I-D.ietf-detnet-bounded-latency}}, Section 6.6. 

CQF benefits for Deterministic QoS include the tightly bounded jitter it provides as well as the per-flow stateless operation, minimizing the complexity of high-speed hardware implementations and allowing to support on transit hops arbitrary number of DetNet flow in the forwarding plane because of the absence of per-hop, per-flow QoS processing. In the terms of the IETF QoS architecture, CQF can be called DiffServ QoS technology, operating only on a traffic aggregate.

CQFs is limited to only limited-scale wide-area network deployments because it cannot take the propagation latency of links into account, nor potential variations thereof. It also requires very high precision clock synchronization, which is uncommon in wide-area network equipment beyond mobile network fronthaul. See {{I-D.eckert-detnet-bounded-latency-problems}} for more details.

This specification introduces and utilizes an enhanced form of CQF where packets are tagged with a cycle identifier, and a limited number of cycles, e.g.: 3...7 are used to overcome these distance and clock synchronization limitations. Because this memo defines how to use the TC field of MPLS LSE as the tag to carry the cycle identifier, it calls this scheme TC Tagged multiple buffer CQF (TC TCQF). See {{I-D.qiang-DetNet-large-scale-DetNet}} and {{I-D.dang-queuing-with-multiple-cyclic-buffers}} for more details of the theory of operations of TCQF. Note that TCQF is not necessarily limited to deterministic operations but could also be used in conjunction with congestion controlled traffic, but those considerations are outside the scope of this memo.

TCQF is likely especially beneficial when MPLS networks are designed to avoid per-hop, per-flow state even for traffic steering, which is the case for networks using SR-MPLS {{RFC8402}} for traffic steering of MPLS unicast traffic and/or BIER-TE {{I-D.ietf-bier-te-arch}} for tree engineering of MPLS multicast traffic. In these networks, it is specifically undesirable to require per-flow signaling to P-LSR solely for DetNet QoS because such per-flow state is unnecessary for traffic steering and would only be required for the bounded latency QoS mechanism and require likely even more complex hardware and manageability support than what was previously required for per-hop steering state (e.g. In RSVP-TE). Note that the DetNet architecture {{RFC8655}} does not include full support for this DiffServ model, which is why this memo describes how to use MPLS TC TCQF with the DetNet architecture per-hop, per-flow processing as well as without it.

# Using TCQF in the DetNet Architecture and MPLS forwarding plane (informative)

This section gives an overview of how the operations of TCQF relates
to the DetNet architecture. We first revisit QoS with DetNet in the absence of TCQF.

     DetNet MPLS       Relay       Transit         Relay       DetNet MPLS
     End System        Node         Node           Node        End System
        T-PE1          S-PE1        LSR-P          S-PE2       T-PE2
     +----------+                                             +----------+
     |   Appl.  |<------------ End-to-End Service ----------->|   Appl.  |
     +----------+   +---------+                 +---------+   +----------+
     | Service  |<--| Service |-- DetNet flow --| Service |-->| Service  |
     +----------+   +---------+  +----------+   +---------+   +----------+
     |Forwarding|   |Fwd| |Fwd|  |Forwarding|   |Fwd| |Fwd|   |Forwarding|
     +-------.--+   +-.-+ +-.-+  +----.---.-+   +-.-+ +-.-+   +---.------+
             :  Link  :    /  ,-----.  \   : Link :    /  ,-----.  \
             +........+    +-[ Sub-  ]-+   +......+    +-[ Sub-  ]-+
                             [Network]                   [Network]
                              `-----'                     `-----'
             |<- LSP -->| |<-------- LSP -----------| |<--- LSP -->|
  
             |<----------------- DetNet MPLS --------------------->|
{: #FIG-DetNet-MPLS title="A DetNet MPLS Network"}


The above {{FIG-DetNet-MPLS}}, is copied from {{RFC8964}}, Figure 2, 
and only enhanced by numbering the nodes to be able to better refer
to them in the following text.

Assume a DetNet flow is sent from T-PE1 to T-PE2 across S-PE1, LSR, S-PE2. 
In general, bounded latency QoS processing is then required on the
outgoing interface of T-PE1 towards S-PE1, and any further outgoing
interface along the path. When T-PE1 and S-PE2 know that their next-hop
is a service LSR, their DetNet flow label stack may simply have the
DetNet flows Service Label (S-Label) as its Top of Stack (ToS) LSE,
explicitly indicating one DetNet flow.

On S-PE1, the next-hop LSR is
not DetNet aware, which is why S-PE1 would need to send a label stack
where the S-Label is followed by a Forwarding Label (F-Label), and
LSR-P would need to perform bounded latency based QoS on that F-Label.

For bounded latency QoS mechanisms relying on per-flow regulator state,
such as in {{TSN-ATS}}, this requires the use of a per-detnet flow F-Label
across the network from S-PE1 to S-PE2, for example through RSVP-TE {{RFC3209}}
enhanced as necessary with QoS parameters matching the underlying bounded
latency mechanism (such as {{TSN-ATS}}).

With TC TCQF, a sequence of LSR and DetNet service node implements
TC TCQF, ideally from T-PE1 (ingress) to T-PE2 (egress).  The ingress
node needs to perform per-DetNet-flow per-packet "shaping" to  assign
each packet of a flow to a particular TCQF cycle. This ingress-edge-function
is currently out of scope of this document (TBD), but would be based
on the same type of edge function as used in CQF.

All LSR/Service node after the ingress node only have to map a
received TCQF tagged DetNet packet to the configured cycle
on the output interface, not requiring any per-DetNet-flow QoS state.
These LSR/Service nodes do therefore also not require per-flow
interactions with the controller plane for the purpose of bounded latency.

Per-flow state therefore is therefore only required on nodes that are 
DetNet service nodes, or when explicit, per-DetNet flow steering
state is desired, instead of ingress steering through e.g.: SR-MPLS.

Operating TCQF per-flow stateless across a service node, such as S-PE1, S-PE2
in the picture is only an option. It is of course equally feasible to
Have one TCQF domain from T-PE1 to S-PE2, start a new TCQF domain there,
running for example up to S-PE2 and start another one to T-PE2.

A service node must act as an egress/ingress edge of a TCQF domain if it needs
to perform operations that do change the timing of packets other than
the type of latency that can be considered in configuration of TCQF (see [](#calculation)).

For example, if T-PE1 is ingress for a TCQF domain, and T-PE2 is the egress,
S-PE1 could perform the DetNet Packet Replication Function (PRF)  without having to be a TQCF 
edge node as long as it does not introduce latencies not included in the TCQF
setup and the controller plane reserves resources for the multitude of flows
created by the replication taking the allocation of resources in the TCQF cycles into account.

Likewise, S-PE2 could perform the Packet Elimination Function without being
a TCQF edge node as this most likely does not introduce any non-TCQF acceptable
latency - and the controller plane accordingly reserves only for one flow
the resources on the S-PE2->T-PE2 leg.

If on the other hand, S-PE2 was to perform the Packet Reordering Function (PRF), this could
create large peaks of packets when out-of-order packets are released together.
A PRF would either have to take care of shaping out those bursts for the traffic
of a flow to again conform to the admitted CIR/PIR, or else the service node
would have to be a TCQF egress/ingress, performing that shaping itself as an
ingress function.

# TCQF per-flow stateless forwarding (normative) {#forwarding}

## Configuration Data model and tag processing for MPLS TC tags {#model}

The following data model summarizes the configuration parameters
as required for TCQF and discussed in further sections. 'tcqf' 
includes the parameters independent of the tagging on an interface.
'tcqf_tc' describes the parameters for interfaces using MPLS TC tagging.

This configuration model is extensible for interfaces with other tagging,
such as IP/DSCP in other documents.

    # Encapsulation agnostic data
    tcqf 
    +-- uint16 cycles
    +-- uint16 cycle_time
    +-- uint32 cycle_clock_offset
    +-- if_config[oif] # Outgoing InterFace
        +-- uint32 cycle_clock_offset
        +-- cycle_map[iif] # Incoming InterFace
            +--uint8 oif_cycle[iif_cycle]
     
    # MPLS TC tagging specific data
    tcqf_tc[oif]
    +--uint8 tc[oif_cycle]
{: #FIG-Data-Model title="TCQF Configuration Data Model"}

     
## Packet processing 

This section explains the MPLS TCQF packet processing and through
it, introduces the semantic of the objects in {{FIG-Data-Model}}

tcqf contains the router/LSR wide configuration of TCQF parameters,
independent of the specific tagging mechanism on any interface. Any
interface can have a different tagging method.

The model represents a single TQCF domain, which is a set of
interfaces acting both as ingress (iif) and egress (oif)
interfaces, capable to forward TCQF packets amongst each other. A router/LSR
may have multiple TCQF domains each with a set of interfaces disjoint
from those of any other TCQF domain.

tcqf.cycles is the number of cycles used across all interfaces in the TCQF domain.
router/LSR MUST support 3 and 4 cycles. To support interfaces with MPLS TC tagging,
7 or less cycles MUST be used across all interfaces in the CQF domain.

The unit of tcqf.cycle_time is micro-seconds.
router/LSR MUST support configuration of cycle-times of 20,50,100,200,500,1000,2000 usec.

Cycles start at an offset of tcqf.cycle_clock_offset in units of nsec as follows. 
Let clock1 be a timestamp of the local reference clock for TCQF, at which
cycle 1 starts, then:

tcqf.cycle_clock_offset = (clock1 mod (tcqf.cycle_time * tcqf.cycles) )

The local reference clock of the LSR/router is expected to be synchronized with
the neighboring LSR/router in TCQF domain.  tcqf.cycle_clock_offset can be configurable
by the operator, or it can be read-only. In either case will the operator be
able to configure working TCQF forwarding through appropriately calculated
cycle mapping.

tcqf.if_config\[oif\] is optional per-interface configuration of TCQF parameters.
tcqf.if_config\[oif\].cycle_clock_offset may be different from tcqf.cycle_clock_offset,
for example, when interfaces are on line cards with independently synchronized clocks,
or when non-uniform ingress-to-egress propagation latency over a complex router/LSR
fabric makes it beneficial to allow per-egress interface or line card configuration
of cycle_clock_offset. It may be configurable or read-only.

The value of -1 for tcqf.if_config\[oif\].cycle_clock_offset is used to indicate
that the domain wide tcqf.cycle_clock_offset is to be used for oif.
This is the only permitted negative number for this parameter.

When a packet is received from iif with a cycle value of iif_cycle and the
packet is routed towards oif, then the cycle value (and buffer) to use on
oif is tcqf.if_config\[oif\].cycle_map\[iif\].oif_cycle\[iif_cycle\]. This is
called the cycle mapping and is must be configurable. This cycle mapping
always happens when the packet is received with a cycle tag on an interface
in a TCQF domain and forwarded to another interface in the same TCQF domain.

tcqf_tc\[oif\].tc\[oif_cycle\] defines how to map from the internal cycle number
oif_cycle to an MPLS TC value on interface oif. When tcqf_tc\[oif\] is
configured, oif will use MPLS TC tagging for TCQF. This mapping
not only used to map from internal cycle number to MPLS TC tag when sending
packets, but also to map from MPLS TC tag to the internal cycle number when
receiving packets.

## TCQF with label stack operations

In the terminology of {{RFC3270}}, TCQF QoS as defined here, is 
TC-Inferred-PSC LSP (E-LSP) behavior: Packets are determined to
belong to the TCQF PSC solely based on the TC of the received
packet.

The internal cycle number SHOULD be assigned from the Top of Stack (ToS)
MPLS label TC bits before any other label stack operations
happens. On the egress side, the TC value of the ToS MPLS label
SHOULD be assigned from the internal cycle number after any label
stack processing.

With this order of processing, TCQF can support forwarding of
packets with any label stack operations such as label swap in the
case of LDP or RSVP-TE created LSP, or no label changes from SID
hop-by-hop forwarding and/or SID/label pop as in the case
of SR-MPLS traffic steering.

## TCQF Pseudocode (normative)

The following pseudocode restates the forwarding behavior of {{forwarding}}
in an algorithmic fashion as pseudocode. It uses the objects of the TCQF configuration
data model defined in [](#model).

    void receive(pak) {
      // Receive side TCQF - retrieve cycle of received packet
      // from packet internal header
      iif = pak.context.iif
      if (tcqf.if_config[iif]) { // TCQF enabled on iif
        if (tcqf_tc[iif]) {      // MPLS TCQF enabled on iif
          tc = pak.mpls_header.lse[tos].tc
          pak.context.tcqf_cycle = map_tc2cycle( tc, tcqf_tc[iif])
        } else // other future encap/tagging options for TCQF
      }
      forward(pak);
    }

    // ... Forwarding including any label stack operations

    void forward(pak) {
      oif = pak.context.oif = forward_process(pak)

      if(ingres_flow_enqueue(pak))
        return // ingress packets are only enqueued here.

      if(pak.context.tcqf_cycle && // non TCQF packets cycle is 0
         tcqf.if_config[oif]) {    // TCQF enabled
        // Map tcqf_cycle iif to oif
        cycle = pak.context.tcqf_cycle
              = map_cycle(cycle,
                tcqf.if_config[oif].cycle_map[[iif])

        if(tcqf.mpls_tc_tag[iif]) { // TC-TCQF
          pak.mpls_header.lse[tos].tc =
            map_cycle2tc(cycle, tcqf_tc[oif])
        } else // other future encap/tagging options for TCQF

        tcqf_enqueue(pak, oif.cycleq[cycle])
      }
    }

    // Started when TCQF is enabled on an interface
    // dequeues packets from oif.cycleq
    void send_tcqf(oif) {
      cycle = 1
      cc =  tcqf.cycle_time *
            tcqf.cycle_time
      o =   tcqf.cycle_clock_offset
      nextcyclestart = floor(tnow / cc) * cc + cc + o

      while(1) {
        ingres_flow_2_tcqf(oif,cycle)
        while(tnow < nextcyclestart) { }
        while(pak = dequeue(oif.cycleq(cycle)) {
          send(pak)
        } 
        cycle = (cycle + 1) mod tcqf.cycles + 1
        nextcyclestart += tcqf.cycle_time
      }
    }
{: #FIG-Pseudocode title="TCQF Pseudocode"}

Processing of ingress DetNet packets is performed
via ingres_flow_enqueue(pak) and
ingres_flow_2_tcqf(oif,cycle) as explained in {{ingres_pseudocode}}.

# TCQF Per-flow Ingress forwarding (normative) {#ingress}

Ingress flows in the context of this text
are packets of flows that enter the router from a non-TCQF interface
and need to be forwarded to an interface with TCQF.

In the most simple case, these packets are sent by the
source and the router is the first-hop router.
In another case, the routers ingress interface
connects to a hop where the previous router(s) did
perform a different bounded latency forwarding mechanism
than TCQF.

## Ingress Flows Configuration Data Model

    # Extends above defined tcqf
    tcqf
    ...
    | Ingress Flows, see below (TBD:
    +-- iflow[flowid]
        +-- uint32 csize # in bits
{: #FIG-IData-Model title="TCQF Ingress Configuration Data Model"}


The data model shown in {{FIG-IData-Model}} expands the tcqf data
model  from {{FIG-Data-Model}}. For every DetNet flow for which
this router is the TCQF ingress, the controller plane has to specify a maximum 
number of bits called csize (cycle size) that are permitted to 
go into each individual cycle. 

Note, that iflow\[flowid\].csize is not specific to the sending
interface because it is a property of the DetNet flow.

## Ingress Flows Pseudocode {#ingres_pseudocode}

When a TCQF ingress is received, it first has to be enqueued into a
per-flow queue. This is necessary because the permitted
burst size for the flow may be larger than what can fit
into a single cycle, or even into the number of cycles
used in the network.

    bool ingres_flow_enqueue(pak) {
      if(!pak.context.tcqf_cycle &&
          flowid = match_detnetflow(pak)) {
        police(pak) // according to RFC9016 5.5
        enqueue(pak, flowq[oif][flowid])
        return true
      }
      return false
    }
{: #FIG-Ingress-enqueue title="TCQF Ingress Enqueue Pseudocode"}


ingres_flow_enqueue(pak) as shown in {{FIG-Ingress-enqueue}}
performs this enqueuing of the packet. Its position
in the DetNet/TCQF forwarding code is shown in 
{{FIG-Pseudocode}}.

police(pak): If the router is not only the TCQF ingress router, but also
the first-hop router from the source, ingres_flow_enqueue(pak)
will also be the place where policing of the flows packet
according to the Traffic Specification of the flow would happen -
to ensure that packets violating the Traffic Specification
will not be forwarded, or be forwarded with lower priority
(e.g.: as best effort). This policing and resulting forwarding
action is not specific to TCQF and therefore out of scope for
this text. See {{RFC9016}}, section 5.5.

    void ingres_flow_2_tcqf(oif, cycle) {
      foreach flowid in flowq[oif][*] {
        free = tcqf.iflow[flowid].csize
        q = flowq[oif][flowid]
        while(notempty(q) &&
              (l = head(q).size) <= free) {
          pak = dequeue(q)
          free -= l
          tcqf_enqueue(pak, oif.cycleq[cycle])
        }
      }
    }
{: #FIG-Ingress-2-TCQF title="TCQF Ingress Pseudocode"}


ingres_flow_2_tcqf(oif, cycle) as shown in
{{FIG-Ingress-2-TCQF}} transfers ingress DetNet flow
packets from their per-flow queue into the queue of the cycle
that will be sent next. The position of ingres_flow_2_tcqf() 
in the DetNet/TCQF forwarding code is shown in {{FIG-Pseudocode}}.

# Implementation, Deployment, Operations and Validation considerations (informative)

## High-Speed Implementation

High-speed implementations with programmable forwarding planes of TCQF
packet forwarding requires Time-Gate Queues for the cycle queues,
such as introduced by {{IEEE802.1Qbv}} and also employed in CQF {{IEEE802.1Qch}}.

Compared to CQF, the accuracy of clock synchronization across the nodes
is reduced as explained in {{calculation}} below.

High-speed forwarding for ingress packets as specified in {{ingress}}
above would require to pass packets first into a per-flow queue and
then re-queue them into a cycle queue. This is not ideal for
high speed implementations.  The pseudocode for ingres_flow_enqueue()
and ingres_flow_2_tcqf(), like the rest of the pseudocode in this
document is only meant to serve as the most compact and hopefully
most easy to read specification of the desired externally observable
behavior of TCQF - but not as a guidance for implementation, especially not
for high-speed forwarding planes.

High-speed forward could be implemented with single-enqueueing into
cycle queues as follows:

Let B\[f\] be the maximum amount of data that the router would need to
buffer for ingress flow f at any point in time. This can be calculated
from the flows Traffic Specification. For example, when using the
parameters of {{RFC9016}}, section 5.5.

    B[f] <= MaxPacketsPerInterval*MaxPayloadSize*8
    
    maxcycles = max( ceil( B[f] / tcqf.iflow[f].csize) | f)

Maxcycles is the maximum number of cycles required so that packets
from all ingress flows can be directly enqueued into maxcycles queues.
The router would then not cycle across tcqf.cycles number of queues,
but across maxcycles number of queues, but still cycling across tcqf.cycles
number of cycle tags.

Calculation of B\[f\] and in result maxcycles may further be refined (lowered)
by additionally known constraints such as the bitrates of the ingress interface(s)
and TCQF output interface(s).

## Controller plane computation of cycle mappings {#calculation}

The cycle mapping is computed by the controller plane by taking at minimum
the link, interface serialization and node internal forwarding latencies as well
as the cycle_clock_offsets into account.

    Router  . O1
     R1     . | cycle 1 | cycle 2 | cycle 3 | cycle 1 |
            .    .
            .     ............... Delay D
            .                    .
            .                    O1'
            .                     | cycle 1 |
    Router  .   | cycle 1 | cycle 2 | cycle 3 | cycle 1 |
      R2    .   O2
    
    CT  = cycle_time
    C   = cycles
    CC  = CT * C
    O1  = cycle_clock_offset router R1, interface towards R2
    O2  = cycle_clock_offset router R2, output interface of interest
    O1' = O1 + D
{: #Calc1 title="Calculation reference"}


Consider in {{Calc1}} that Router R1 sends packets via C = 3 cycles with a
cycle_clock offset of O1 towards Router R2. These packets arrive
at R2 with a cycle_clock offset of O1' which includes through D all latencies
incurred between releasing a packet on R1 from the cycle buffer until
it can be put into a cycle buffer on R2: serialization delay on R1,
link delay, non_CQF delays in R1 and R2, especially forwarding in
R2, potentially across an internal fabric to the output interface
with the sending cycle buffers.

    A = ( ceil( ( O1' - O2 ) / CT) + C + 1) mod CC
    map(i) = (i - 1 + A) mod C + 1
{: #Calc2 title="Calculating cycle mapping"}


{{Calc2}} shows a formula to calculate the cycle mapping between
R1 and R2, using the first available cycle on R2. In the example
of {{Calc1}} with CT = 1, (O1' - O2) =~ 1.8, A will be 0, resulting
in map(1) to be 1, map(2) to be 2 and map(3) to be 3.

The offset "C" for the calculation of A is included so that
a negative (O1 - O2) will still lead to a positive A.
   
In general, D will be variable \[Dmin...Dmax\], for example because
of differences in serialization latency between min and max size
packets, variable link latency because of temperature based length
variations, link-layer variability (radio links) or in-router
processing variability. In addition, D also needs to account for the
drift between the synchronized clocks for R1 and R2. This
is called the Maximum Time Interval Error (MTIE).

Let A(d) be A where O1' is calculated with D = d. To account for
the variability of latency and clock synchronization, map(i) has to
be calculated with A(Dmax), and the controller plane needs to
ensure that that A(Dmin)...A(Dmax) does cover at most (C - 1) cycles.

If it does cover C cycles, then C and/or CT are chosen too small,
and the controller plane needs to use larger numbers for either.

This (C - 1) limitation is based on the understanding that there is only
one buffer for each cycle, so a cycle cannot receive packets
when it is sending packets. While this could be changed by
using double buffers, this would create additional implementation
complexity and not solve the limitation for all cases, because
the number of cycles to cover \[Dmin...Dmax\] could also be (C + 1)
or larger, in which case a tag of 1...C would not suffice. 

## Link speed and bandwidth sharing

TCQF hops along a path do not need to have the same bitrate, they
just need to use the same cycle time. The controller plane has to then
be able to take the TCQF capacity of each hop into account when
admitting flows based on their Traffic Specification and TCQF csize. 

TCQF does not require to be allocated 100% of the
link bitrate. When TCQF has to share a link with other traffic
classes, queuing just has to be set up to ensure that all
data of a TCQF cycle buffer can be sent within the TCQF
cycle time. For example by making the TCQF cycle queues the
highest priority queues and then limiting their capacity through
admission control to leave time for other queues to be served
as well.

## Validation

{{LDN}} describes an experimental validation of TCQF with high-speed forwarding
hardware and provides further details on the mathematical models.

# Security Considerations

TBD. 

# IANA Considerations

This document has no IANA considerations.

# Changelog

00

Initial version

01 

Added new co-author.

Changed Data Model to "Configuration Data Model",

and changed syntax from YANG tree to a non-YANG tree, removed empty section
targeted for YANG model. Reason: the configuration parameters that we need
to specify the forwarding behavior is only a subset of what likely would
be a good YANG model, and any work to define such a YANG model not necessary
to specify the algorithm would be scope creep for this specification. Better
done in a separate YANG document. 
Example additional YANG aspects for such a document are
how to map parameters to configuration/operational space, what additional
operational/monitoring parameter to support and how to map the
YANG objects required into various pre-existing YANG trees.

Improved text in forwarding section, simplified sentences, used simplified
configuration data model.

02

Refresh

03

Added ingress processing, and further implementation considerations.
